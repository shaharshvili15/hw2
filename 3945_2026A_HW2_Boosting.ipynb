{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10316d5779a3733",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Exercise 2: Boosting\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "\n",
    "* The homework assignments are executed automatically. \n",
    "* Failure to comply with the following instructions will result in a significant penalty. \n",
    "* Appeals regarding your failure to read these instructions will be denied. \n",
    "* Kind reminder: the homework assignments contribute 60% of the final grade.\n",
    "\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/).\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes a zip file that contains this notebook, with your ID as the file name. For example, `hw1_123456789_987654321.zip` if you submitted in pairs and `hw1_123456789.zip` if you submitted the exercise alone. The name of the notebook should follow the same structure.\n",
    "   \n",
    "Please use only a **zip** file in your submission.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Please sign that you have read and understood the instructions: \n",
    "\n",
    "### *** YOUR RUNI EMAILS HERE ***\n",
    "shahar.shvili@post.runi.ac.il,itay.baror@post.runi.ac.il\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "735832cbfa43f83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877cca-f7d1-41ca-9d3d-28d587bec85c",
   "metadata": {},
   "source": [
    "# Design your algorithm\n",
    "Make sure to describe the algorithm, its limitations, and describe use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7760b-b4ef-45a9-9aad-88b45fd8d442",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "when using Adaboost we are given feature vectors \n",
    "$x_i \\in \\mathbb{R}^d$ \n",
    "with binary lables \n",
    "$y_i \\in \\{-1, +1\\}$ \n",
    "our goal is to combine many weak learners classifiers into a strong classifier with low traning error. the algorithm achieves this by iterating over the stumps and adding the ones that make the model better and adjusting the weights of the samples according to if they are missclassified or not \n",
    "\n",
    "**Algorithem**\n",
    "\n",
    "let n be the number of samples, and T the number of boosting rounds\n",
    "\n",
    "**Step 1: Initialization**\n",
    "\n",
    "initialize sample weights for every \n",
    "$i=1,...,n$ $w_i=1/n$ \n",
    "at the beginning all samples  are equally as important as we continue the weights will change acording to if we misclassifed the samples\n",
    "initialize the ensemble prediction function \n",
    "$H_0=0$ \n",
    "we start with no classifier while we build the model we will add stumps to here \n",
    "\n",
    "**Step 2:**\n",
    "\n",
    "For t=0,....,T-1\n",
    "each iteration adds one weak learner and updates the weights \n",
    "\n",
    "**Step 2.1: Train the weak learner**\n",
    "\n",
    "Train the weak leaner using the current weights. this means that samples with larger weights have more influence on the traning. \n",
    "\n",
    "**Step 2.2: Compute weighted classification error**\n",
    "\n",
    "$\\epsilon = \\sum_{i : h(x_i) \\neq y_i} w_i$\n",
    "\n",
    "calculate how good the classification did in the weighted dataset  \n",
    "- if $\\epsilon<=0.5$ then we got to the end since our current weak learner does not add to our classification \n",
    "- if not continue to the next step \n",
    "\n",
    "**Step 2.3: Compute the weight of the weak learner**\n",
    "\n",
    "$\\alpha = \\frac{1}{2} \\ln\\!\\left(\\frac{1-\\epsilon}{\\epsilon}\\right)$\n",
    "\n",
    "when weak learners have low error then we want them to matter more so there weight will be large \n",
    "\n",
    "**Step 2.4: Update the model with the current weak learner**\n",
    "\n",
    "add the current weak learner to the model of all the weak learners we add up until now \n",
    "$H_{t+1} = H_t + \\alpha h$\n",
    "\n",
    "**Step 2.5: Update the weights**\n",
    "update the weights of samples that are currently in the model \n",
    "\n",
    "$w_i \\leftarrow \\frac{w_i e^{-\\alpha h(x_i) y_i}}{2\\sqrt{\\epsilon (1-\\epsilon)}}$\n",
    "\n",
    "misclassified samples get higher weights correctly classified ones get lower weights \n",
    "\n",
    "**Step 3: return the final model**\n",
    "\n",
    "return $H(x) = \\operatorname{sign}\\!\\left( \\sum_{t=1}^T \\alpha_t h_t(x) \\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158ff2629daf2bb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Your implementations\n",
    "You may add new cells, write helper functions or test code as you see fit.\n",
    "Please use the cell below and include a description of your implementation.\n",
    "Explain code design consideration, algorithmic choices and any other details you think is relevant to understanding your implementation.\n",
    "Failing to explain your code will lead to point deductions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe206c9-d1f4-440b-aca0-c807cdd79451",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b85d8f7447ebce0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# # Part 1: Implementing AdaBoost\n",
    "# class AdaBoostCustom:\n",
    "#     def __init__(self, T):\n",
    "#         self.T = T\n",
    "#         self.alphas = []\n",
    "#         self.models = []\n",
    "#         self.classes = None\n",
    "#         # Note: You may add more attributes\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         # Your code here\n",
    "#         pass\n",
    "    \n",
    "#     def predict(self, X):  \n",
    "#         # Your code here\n",
    "#         pass\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "class AdaBoostCustom:\n",
    "    def __init__(self, T=100, base_estimator=None, learning_rate=1.0, random_state=None):\n",
    "        self.T = int(T)\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.random_state = random_state\n",
    "        self.base_estimator = base_estimator or DecisionTreeClassifier(\n",
    "            max_depth=1, random_state=random_state\n",
    "        )\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "        self.neg_class = None\n",
    "        self.pos_class = None\n",
    "\n",
    "    # Encode to {-1,+1} by picking a deterministic positive/negative class.\n",
    "    def encode_pm1(self, y):\n",
    "        y = np.asarray(y)\n",
    "        classes = np.unique(y)\n",
    "        if classes.size != 2:\n",
    "            raise ValueError(\"AdaBoostCustom supports binary classification only.\")\n",
    "        self.neg_class, self.pos_class = classes[0], classes[1]\n",
    "        return np.where(y == self.pos_class, 1, -1)\n",
    "\n",
    "    # Decode {-1,+1} back to original labels.\n",
    "    def decode_labels(self, y_pm1):\n",
    "        return np.where(y_pm1 >= 0, self.pos_class, self.neg_class)\n",
    "\n",
    "    # Core boosting loop with weighted trees, SAMME alpha, and weight renormalization.\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y_pm1 = self.encode_pm1(y).astype(float)\n",
    "\n",
    "        n = X.shape[0]\n",
    "        D = np.full(n, 1.0 / n)\n",
    "        eps = 1e-12\n",
    "\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "\n",
    "        for t in range(self.T):\n",
    "            stump = DecisionTreeClassifier(**self.base_estimator.get_params())\n",
    "            stump.fit(X, y_pm1, sample_weight=D)\n",
    "\n",
    "            h_pm1 = stump.predict(X).astype(float)\n",
    "            err = float(np.sum(D * (h_pm1 != y_pm1)))\n",
    "            err = float(np.clip(err, 0.0, 1.0))\n",
    "\n",
    "            if err >= 0.5 - 1e-15:\n",
    "                if t == 0:\n",
    "                    self.alphas.append(1e-6)\n",
    "                    self.models.append(stump)\n",
    "                break\n",
    "\n",
    "            alpha = 0.5 * np.log((1.0 - err) / (err + eps))\n",
    "            alpha *= self.learning_rate\n",
    "\n",
    "            D *= np.exp(-alpha * y_pm1 * h_pm1)\n",
    "            s = D.sum()\n",
    "            if s == 0.0:\n",
    "                self.alphas.append(alpha)\n",
    "                self.models.append(stump)\n",
    "                break\n",
    "            D /= s\n",
    "\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(stump)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Margin F(x) = sum_t alpha_t h_t(x).\n",
    "    def decision_function(self, X):\n",
    "        if not self.models:\n",
    "            raise ValueError(\"Model is not fitted.\")\n",
    "        X = np.asarray(X)\n",
    "        F = np.zeros(X.shape[0], dtype=float)\n",
    "        for alpha, stump in zip(self.alphas, self.models):\n",
    "            F += alpha * stump.predict(X).astype(float)\n",
    "        return F\n",
    "\n",
    "    # Final classifier H(x) = sign(F(x)); ties go to +1.\n",
    "    def predict(self, X):\n",
    "        F = self.decision_function(X)\n",
    "        y_pm1 = np.sign(F)\n",
    "        y_pm1[y_pm1 == 0] = 1\n",
    "        return self.decode_labels(y_pm1.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24f179351fa008",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generate data\n",
    "Please use the cell below to discuss your dataset choice and why it is appropriate (or not) for this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4083f-5267-44d3-89ed-65864f82aa57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a14a3b8890e86f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simple_dataset(n=800, class_sep=2.0, test_size=0.3, random_state=0):\n",
    "    X, y = make_classification(\n",
    "        n_samples=n, n_features=2, n_informative=2, n_redundant=0,\n",
    "        n_clusters_per_class=1, class_sep=class_sep, flip_y=0.0,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return train_test_split(X, y, test_size=test_size, stratify=y, random_state=random_state)\n",
    "\n",
    "\n",
    "# --- generate separable dataset ---\n",
    "X_train, X_test, y_train, y_test = simple_dataset(n=800, class_sep=2.0, test_size=0.3, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49bb42f79a55f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# AdaBoost demonstration \n",
    "Demonstrate your AdaBoost implementation.\n",
    "\n",
    "Add plots and figures. \n",
    "\n",
    "Please use the cell below to describe your results and tests.\n",
    "\n",
    "Describe the difference between your implementation and the sklearn implementation. Hint: you can look at the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064afb5-aeea-48d8-b315-921bf4f8238f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b3628856e1335fd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom AdaBoost Accuracy: 0.9917\n",
      "Sklearn AdaBoost Accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "custom_model = AdaBoostCustom(T=10)\n",
    "sklearn_model = AdaBoostClassifier(n_estimators=10, random_state=42)\n",
    "\n",
    "# Your code here\n",
    "custom_model.fit(X_train, y_train)\n",
    "sklearn_model.fit(X_train, y_train) \n",
    "# Predictions\n",
    "y_pred_custom = custom_model.predict(X_test)\n",
    "y_pred_sklearn = sklearn_model.predict(X_test)\n",
    "# Accuracy\n",
    "acc_custom = accuracy_score(y_test, y_pred_custom)\n",
    "acc_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Custom AdaBoost Accuracy: {acc_custom:.4f}\")\n",
    "print(f\"Sklearn AdaBoost Accuracy: {acc_sklearn:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73fa2fceedc77e92",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Generate additional data sets\n",
    "Generate at least two experimental datasets with binary labels, designed to demonstrate specific properties of AdaBoost (e.g., handling noise or overfitting).\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below to describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34701c-cc3b-439a-b2d0-393449cf5a20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d38dc132b23e7b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate additional data sets\n",
    "\n",
    "# Split data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "567bfcc7-6542-489d-b67a-13004d3103b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5667598e-28fa-496e-b04b-ae528e653894",
   "metadata": {},
   "source": [
    "# Test algorithms\n",
    "Test your AdaBoost, a library implementation of AdaBoost and at least two additional models, one of which must be another boosting algorithm on your two datasets.\n",
    "\n",
    "Add plots and figures.\n",
    "\n",
    "Please use the cell below to describe your suggested approach in detail. Use formal notations where appropriate.\n",
    "\n",
    "Describe and discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f959a354-350d-49b8-b203-bdf889778766",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e4c3e86-003e-4812-8b36-65e13fccc4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d097c906-f362-4084-9ee8-0ff34568d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c95c7f-d3a9-4e3d-b539-02e020358766",
   "metadata": {},
   "source": [
    "# Use of generative AI\n",
    "Please use the cell below to describe your use of generative AI in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36753fd7-8b2d-487b-82ae-dc318eca3ca6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
